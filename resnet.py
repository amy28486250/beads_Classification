# -*- coding: utf-8 -*-
"""ResNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zXatdzCm2HbUV-h-VrhiGviA_Cci78Ou
"""

#更改tf版本跟本機 2.10.0相同
!pip uninstall tensorflow
!pip install tensorflow==2.10
!pip install tensorflow-gpu==2.10
!pip install --upgrade 'google-auth-oauthlib<0.5,>=0.4.1'

!pip install --upgrade 'protobuf==3.19.6'
!pip install --upgrade 'tensorboard==2.10.0'

import gast
import google
import keras
import tensorboard
import tensorflow as tf
#確認目前tf的版本
#import tensorflow as tf
print(tf.__version__)
print(tf.config.list_physical_devices('GPU'))

import torch
import torch.nn as nn
import torch.nn.functional as F


#from torchvision import datasets, transforms

import os
import numpy as np
import shutil
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau
from keras.layers import Activation
from keras.regularizers import l1
import matplotlib.pyplot as plt
import math


from tensorflow.keras.layers import Input, Add, GlobalAveragePooling2D

from google.colab import drive
drive.mount('/content/drive')

# Define main directory
path = '/content/drive/MyDrive/AI'


train_path = path + '/train7_5'
test_path = path + '/test6'

#改了
model_path = path + '/model/multi_MgPoly_modal_ResNet_6v07_tf_2_10'

# Create empty lists to store loss values
training_losses = []
validation_losses = []

datagen_train = ImageDataGenerator(
        rescale=1./255,
        validation_split=0.2,
        rotation_range=10,  # Random rotation
        width_shift_range=0.1,  # Random horizontal shift
        height_shift_range=0.1,  # Random vertical shift
        horizontal_flip=True,  # Random horizontal flip
        vertical_flip=True  # Random vertical flip
    )

# Prepare iterators for training and validation datasets
train_it = datagen_train.flow_from_directory(train_path,
                                  class_mode='categorical', batch_size=64, target_size=(80, 80), subset='training')
valid_it = datagen_train.flow_from_directory(train_path,
                                  class_mode='categorical', batch_size=64, target_size=(80, 80), subset='validation')

def lr_schedule(epoch):
    initial_learning_rate = 0.001
    drop = 0.5  # Learning rate drops by halfS
    epochs_drop = 3  # Drop the learning rate every 10 epochs

    lr = initial_learning_rate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    #lr = initial_learning_rate * math.exp(math.floor((1 + epoch) / epochs_drop))

    return lr


# Create a LearningRateScheduler callback
lr_scheduler = LearningRateScheduler(lr_schedule)

def identity_block(x, filters, block_name):
    # 定義 ResNet 恒等塊
    filters1, filters2, filters3 = filters

    x_shortcut = x

    x = Conv2D(filters1, (1, 1), padding='valid', name=block_name+'_conv1')(x)
    x = BatchNormalization(name=block_name+'_bn1')(x)
    x = Activation('relu', name=block_name+'_relu1')(x)

    x = Conv2D(filters2, (3, 3), padding='same', name=block_name+'_conv2')(x)
    x = BatchNormalization(name=block_name+'_bn2')(x)
    x = Activation('relu', name=block_name+'_relu2')(x)

    x = Conv2D(filters3, (1, 1), padding='valid', name=block_name+'_conv3')(x)
    x = BatchNormalization(name=block_name+'_bn3')(x)

    x = Add(name=block_name+'_add')([x, x_shortcut])
    x = Activation('relu', name=block_name+'_relu3')(x)

    return x

def convolutional_block(x, filters, strides, block_name):
    # 定義 ResNet 卷積塊
    filters1, filters2, filters3 = filters
    #filters1, filters3 = filters
    x_shortcut = x

    x = Conv2D(filters1, (1, 1), strides=strides, padding='same', name=block_name+'_conv1')(x)
    x = BatchNormalization(name=block_name+'_bn1')(x)
    x = Activation('relu', name=block_name+'_relu1')(x)

    x = Conv2D(filters2, (3, 3), padding='same', name=block_name+'_conv2')(x)
    x = BatchNormalization(name=block_name+'_bn2')(x)
    x = Activation('relu', name=block_name+'_relu2')(x)


    x = Conv2D(filters3, (1, 1), padding='same', name=block_name+'_conv3')(x)
    x = BatchNormalization(name=block_name+'_bn3')(x)

    x_shortcut = Conv2D(filters3, (1, 1), strides=strides, padding='valid', name=block_name+'_shortcut_conv')(x_shortcut)
    x_shortcut = BatchNormalization(name=block_name+'_shortcut_bn')(x_shortcut)

    x = Add(name=block_name+'_add')([x, x_shortcut])
    x = Activation('relu', name=block_name+'_relu3')(x)

    return x

def ResNet(input_shape=(80, 80, 3), classes=6):
    # 定義 ResNet 模型
    input_tensor = Input(shape=input_shape)

    x = Conv2D(16, (7, 7), strides=(2, 2), padding='valid', name='conv1')(input_tensor)
    x = BatchNormalization(name='bn1')(x)
    x = Activation('relu', name='relu1')(x)
    x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='maxpool1')(x)

    x = convolutional_block(x, filters=[64, 64, 256], strides=(1, 1), block_name='conv_block2')
    #x = convolutional_block(x, filters=[64, 256], strides=(1, 1), block_name='conv_block2')
    x = identity_block(x, filters=[64, 64, 256], block_name='id_block3')


    x = convolutional_block(x, filters=[128, 128, 512], strides=(2, 2), block_name='conv_block4')
    #x = convolutional_block(x, filters=[128, 512], strides=(2, 2), block_name='conv_block4')
    x = identity_block(x, filters=[128, 128, 512], block_name='id_block5')

    x = Dropout(0.3)(x)

    x = convolutional_block(x, filters=[256, 256, 1024], strides=(2, 2), block_name='conv_block6')
    #x = convolutional_block(x, filters=[256, 1024], strides=(2, 2), block_name='conv_block6')
    x = identity_block(x, filters=[256, 256, 1024], block_name='id_block7')



    x = GlobalAveragePooling2D()(x)
    x = Dense(classes, activation='softmax', name='dense_output')(x)

    model = tf.keras.models.Model(inputs=input_tensor, outputs=x, name='ResNet')

    return model
    #這應該是ResNet20...
# 使用 ResNet 函數建立模型
model = ResNet(input_shape=(80, 80, 3), classes=6)
model.summary()

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
# Training loop with the History object
history = model.fit(
        train_it,
        steps_per_epoch=len(train_it),
        validation_data=valid_it,
        validation_steps=len(valid_it),
        epochs=100,
        callbacks=[early_stopping,lr_scheduler]
    )

# Access the training and validation loss values from the History object
training_losses = history.history['loss']
validation_losses = history.history['val_loss']
training_accuracy = history.history['categorical_accuracy']
validation_accuracy = history.history['val_categorical_accuracy']


# Save the final model after training
model.save(model_path)

# Plot the training and validation loss curves
epochs = np.arange(1, len(training_losses) + 1)
plt.figure()
plt.plot(epochs, training_losses, 'bo-', label='Training loss')
plt.plot(epochs, validation_losses, 'ro-', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot the training and validation accuracy curves
epochs = np.arange(1, len(training_accuracy) + 1)
plt.figure()
plt.plot(epochs, training_accuracy, 'bo-', label='Training accuracy')
plt.plot(epochs, validation_accuracy, 'ro-', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#使用結束後請按:
print(":)")