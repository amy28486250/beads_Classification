# -*- coding: utf-8 -*-
"""single_tf_2.10

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vYtfNM25nny0h7WsJkNvUdQ37Y-hbwcy
"""

#更改tf版本跟本機 2.10.0相同
!pip uninstall tensorflow
!pip install tensorflow==2.10
!pip install tensorflow-gpu==2.10

!pip install --upgrade 'google-auth-oauthlib<0.5,>=0.4.1'

!pip install --upgrade 'protobuf==3.19.6'
!pip install --upgrade 'tensorboard==2.10.0'

import gast
import google
import keras
import tensorboard
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping

#確認目前tf的版本
#import tensorflow as tf
print(tf.__version__)
print(tf.config.list_physical_devices('GPU'))

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import shutil
#import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D
from tensorflow.keras.models import load_model

import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.callbacks import EarlyStopping
import math

def lr_schedule(epoch):
    initial_learning_rate = 0.0005
    drop = 0.5  # Learning rate drops by half
    epochs_drop = 2  # Drop the learning rate every 10 epochs

    lr = initial_learning_rate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    #lr = initial_learning_rate * math.exp(math.floor((1 + epoch) / epochs_drop))

    return lr


# Create a LearningRateScheduler callback
lr_scheduler = LearningRateScheduler(lr_schedule)
# Define main directory
path = '/content/drive/MyDrive/AI'

train_path = path + '/train5'
test_path = path + '/experiment_Data/TEST/crop/single'

model_path = '/content/drive/MyDrive/AI/model/singleMgPoly_modal_v02_20e'

if os.path.exists(model_path):
    print("Loading existing model...")
    model = load_model(model_path)
else:
    print("Creating and training a new model...")

    # Create a data generator for training and validation
    datagen_train = ImageDataGenerator(
        rescale=1./255,
        validation_split=0.3,
        rotation_range=45,  # Random rotation
        width_shift_range=0.15,  # Random horizontal shift
        height_shift_range=0.15,  # Random vertical shift
        horizontal_flip=True,  # Random horizontal flip
        vertical_flip=True  # Random vertical flip
    )

    # Prepare iterators for training and validation datasets
    train_it = datagen_train.flow_from_directory(train_path,
                                                 class_mode='categorical', batch_size=64, target_size=(20, 20), subset='training')
    valid_it = datagen_train.flow_from_directory(train_path,
                                                 class_mode='categorical', batch_size=64, target_size=(20, 20), subset='validation')

    # Define the model architecture
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(20, 20, 3)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(train_it.num_classes, activation='softmax'))  # Change here

    # Define the EarlyStopping callback
    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    # Compile and fit the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Change loss here
    history = model.fit(
        train_it,
        steps_per_epoch=len(train_it),
        validation_data=valid_it,
        validation_steps=len(valid_it),
        epochs=100,
        callbacks=[early_stopping,lr_scheduler]
    )

    # Access the training and validation loss values from the History object
    training_losses = history.history['loss']
    validation_losses = history.history['val_loss']
    training_accuracy = history.history['accuracy']  # Change here
    validation_accuracy = history.history['val_accuracy']  # Change here

    # Save the newly trained model
    model.save(model_path)

    # Plot the training and validation loss curves
    epochs = np.arange(1, len(training_losses) + 1)
    plt.figure()
    plt.plot(epochs, training_losses, 'bo-', label='Training loss')
    plt.plot(epochs, validation_losses, 'ro-', label='Validation loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot the training and validation accuracy curves
    epochs = np.arange(1, len(training_accuracy) + 1)
    plt.figure()
    plt.plot(epochs, training_accuracy, 'bo-', label='Training accuracy')
    plt.plot(epochs, validation_accuracy, 'ro-', label='Validation accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

